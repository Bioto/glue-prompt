---
alwaysApply: true
---

# Style Guide: Magical Yet Maintainable

Write code that feels like magic but reads like poetry. Every function should be elegant, every module clear, every abstraction purposeful.

## Core Principles

### 1. Never Patch - Always Fix
- **NO bandaids on broken code** - if it's broken, fix the root cause
- **NO workarounds** - they become permanent fixtures
- **NO "TODO: Fix this properly later"** - there is no later, only now
- **YES proper diagnosis** - understand *why* before applying solutions
- **YES refactoring** - improve the foundation, not the facade

### 2. Readability Is King
- Code is read 10x more than it's written - optimize accordingly
- If you need to think hard about what code does, so will others
- Clever code impresses for a day; clear code serves for years
- Your code should be self-documenting, but not self-encrypting

### 3. Document the Non-Obvious
- **When to document:**
  - Complex algorithms or business logic
  - Non-obvious performance considerations
  - Workarounds for external API quirks
  - "Why" decisions that aren't apparent from code
  - Edge cases and their handling
- **When NOT to document:**
  - What the code clearly shows (don't state the obvious)
  - Redundant docstrings that just restate function names

## Python Style Standards

### Naming Conventions
```python
# Use descriptive names that reveal intent
✅ def transform_user_context_for_llm(context: dict) -> str:
❌ def proc_ctx(c: dict) -> str:

# Constants are SCREAMING_SNAKE_CASE
MAX_RETRY_ATTEMPTS = 3
DEFAULT_TIMEOUT_SECONDS = 30

# Classes are PascalCase
class WorkflowOrchestrator:
    pass

# Functions and variables are snake_case
def calculate_token_usage(messages: list[dict]) -> int:
    total_tokens = 0
```

### Type Hints: Use Them
```python
# Be explicit about types - they're documentation and validation
✅ def process_response(data: dict[str, Any], timeout: int = 30) -> Response:
❌ def process_response(data, timeout=30):

# Use modern Python 3.10+ type syntax
✅ def merge_configs(base: dict, override: dict | None = None) -> dict:
❌ def merge_configs(base: dict, override: Optional[dict] = None) -> dict:
```

### Function Design
```python
# Functions should do ONE thing well
✅
def validate_api_key(key: str) -> bool:
    """Check if API key format is valid."""
    return len(key) > 0 and key.startswith("sk-")

def authenticate_request(api_key: str) -> User:
    """Authenticate and return user for given API key."""
    if not validate_api_key(api_key):
        raise ValueError("Invalid API key format")
    return user_service.get_user_by_key(api_key)

❌
def validate_and_auth(key: str):
    """Validates and authenticates."""  # Does too many things
    if len(key) > 0 and key.startswith("sk-"):
        return user_service.get_user_by_key(key)
    raise ValueError("Bad key")
```

### Error Handling
```python
# Provide context in exceptions
✅
if not api_key:
    raise ValueError(
        "API key is required. Set OPENAI_API_KEY environment variable "
        "or pass api_key parameter."
    )

❌
if not api_key:
    raise ValueError("No API key")

# Don't silence errors without logging
✅
try:
    result = risky_operation()
except SpecificException as e:
    logger.error(f"Operation failed: {e}", exc_info=True)
    # Then either re-raise, return default, or handle appropriately
    raise

❌
try:
    result = risky_operation()
except Exception:
    pass  # Silent failures hide bugs!
```

### Documentation Standards
```python
# Docstrings for public APIs and complex logic
def execute_workflow(
    workflow: Workflow,
    context: dict[str, Any],
    max_retries: int = 3
) -> WorkflowResult:
    """
    Execute a workflow with the given context.

    Orchestrates the workflow execution including retry logic, error handling,
    and result aggregation. Uses exponential backoff for retries.

    Args:
        workflow: The workflow configuration to execute
        context: Initial context dict passed to workflow steps
        max_retries: Maximum number of retry attempts on transient failures

    Returns:
        WorkflowResult containing outputs from all successful steps

    Raises:
        WorkflowExecutionError: If workflow fails after all retries
        ValidationError: If workflow or context is invalid

    Example:
        >>> workflow = Workflow(steps=[...])
        >>> result = execute_workflow(workflow, {"input": "data"})
        >>> print(result.final_output)
    """
```

### Complex Logic Documentation
```python
# Explain the WHY, not the WHAT
✅
# We batch requests to stay under rate limits (10 req/sec)
# and reduce API costs by ~40% through connection reuse
async def batch_process_requests(requests: list[Request]) -> list[Response]:
    batches = chunk_list(requests, size=10)
    # ... implementation

❌
# This function processes requests in batches
async def batch_process_requests(requests: list[Request]) -> list[Response]:
    batches = chunk_list(requests, size=10)  # chunks into size 10
    # ... implementation
```

### Modern Python Patterns
```python
# Use context managers for resource management
✅
async with aiohttp.ClientSession() as session:
    response = await session.get(url)
    return await response.json()

# Use dataclasses for data structures
from dataclasses import dataclass

@dataclass
class Config:
    api_key: str
    timeout: int = 30
    max_retries: int = 3

# Use match/case for complex conditionals (Python 3.10+)
match response.status:
    case 200:
        return response.data
    case 404:
        raise NotFoundError("Resource not found")
    case 429:
        await self._handle_rate_limit()
    case _:
        raise APIError(f"Unexpected status: {response.status}")
```

## Code Organization

### File Structure
- One class per file for complex classes
- Group related utilities in modules
- Keep files under 500 lines when possible
- Use `__init__.py` to expose public APIs

### Import Organization
```python
# Standard library
import asyncio
from typing import Any

# Third-party
import aiohttp
from pydantic import BaseModel

# Local imports
from gluellm.config import Config
from gluellm.models.workflow import Workflow
```

## Project-Specific Patterns

### For GlueLLM
- Workflows should be composable and chainable
- Use hooks for cross-cutting concerns (logging, metrics, tracing)
- Async by default for I/O operations
- Configuration via Pydantic models with validation
- Use structured logging with context

### Testing
- Write tests that document expected behavior
- Test edge cases and error conditions
- Use descriptive test names: `test_workflow_retries_on_rate_limit_error`
- Mock external dependencies, test integration points

## The Review Checklist

Before submitting code, ask:
- [ ] Would I understand this code in 6 months without context?
- [ ] Have I fixed the root cause, not symptoms?
- [ ] Are complex parts documented with "why" not just "what"?
- [ ] Do error messages guide users toward solutions?
- [ ] Are types annotated for clarity?
- [ ] Have I tested edge cases?
- [ ] Does this follow existing project patterns?
- [ ] Would I be proud to show this code to a master craftsperson?

---

*"Magic is just science we don't understand yet. Good code is magic we DO understand."*
